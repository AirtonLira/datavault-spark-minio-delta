{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem Data Vault com PySpark e Delta Lake\n",
    "\n",
    "Este notebook demonstra como implementar uma arquitetura Data Vault usando PySpark e Delta Lake, com o conjunto de dados de E-commerce brasileiro da Olist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuração do Ambiente\n",
    "\n",
    "Primeiro, vamos configurar nossa sessão Spark com suporte ao Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão do Apache Spark: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Configurar a sessão Spark com Delta Lake\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataVaultModeling\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/home/jovyan/data/delta\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Verificar a versão do Spark\n",
    "print(f\"Versão do Apache Spark: {spark.version}\")\n",
    "\n",
    "# Configurações para melhor desempenho\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "spark.conf.set(\"spark.default.parallelism\", \"10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carregar os Dados Brutos do Minio\n",
    "\n",
    "Vamos carregar os dados do conjunto de dados da Olist que estão armazenados no Minio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os caminhos para os arquivos no Minio\n",
    "minio_bucket = \"datalakeprd\"\n",
    "base_path = f\"s3a://{minio_bucket}\"\n",
    "\n",
    "# Carregar os datasets\n",
    "customers_df = spark.read.csv(f\"{base_path}/olist_customers_dataset.csv\", header=True, inferSchema=True)\n",
    "orders_df = spark.read.csv(f\"{base_path}/olist_orders_dataset.csv\", header=True, inferSchema=True)\n",
    "order_items_df = spark.read.csv(f\"{base_path}/olist_order_items_dataset.csv\", header=True, inferSchema=True)\n",
    "products_df = spark.read.csv(f\"{base_path}/olist_products_dataset.csv\", header=True, inferSchema=True)\n",
    "sellers_df = spark.read.csv(f\"{base_path}/olist_sellers_dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Visualizar um pouco dos dados\n",
    "print(\"Amostra da tabela de Clientes:\")\n",
    "customers_df.show(5)\n",
    "\n",
    "print(\"\\nAmostra da tabela de Pedidos:\")\n",
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entendendo o Modelo Data Vault\n",
    "\n",
    "O Data Vault consiste em três tipos principais de tabelas:\n",
    "\n",
    "1. **Hubs**: Armazenam as chaves de negócio e são identificadores únicos das entidades\n",
    "2. **Links**: Armazenam as relações entre os hubs\n",
    "3. **Satellites**: Armazenam os atributos descritivos dos hubs e links\n",
    "\n",
    "Vamos implementar um modelo Data Vault para este cenário de e-commerce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Funções Auxiliares para Data Vault\n",
    "\n",
    "Vamos criar algumas funções para ajudar na criação das tabelas do Data Vault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gerar hash keys para as entidades\n",
    "def generate_hash_key(df, columns, key_name):\n",
    "    \"\"\"Gera uma hash key a partir de uma ou mais colunas.\"\"\"\n",
    "    columns_concat = F.concat_ws(\"|\", *[F.col(c) for c in columns])\n",
    "    return df.withColumn(key_name, F.sha2(columns_concat, 256))\n",
    "\n",
    "# Função para adicionar metadados padrão do Data Vault\n",
    "def add_dv_metadata(df):\n",
    "    \"\"\"Adiciona colunas de metadados padrão do Data Vault.\"\"\"\n",
    "    return df.withColumn(\"load_date\", F.current_timestamp()) \\\n",
    "             .withColumn(\"record_source\", F.lit(\"OLIST_DATASET\"))\n",
    "\n",
    "# Definir o caminho base para as tabelas Delta\n",
    "delta_base_path = \"/home/jovyan/data/delta\"\n",
    "os.makedirs(delta_base_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Criação das Tabelas Hub\n",
    "\n",
    "Vamos criar as seguintes tabelas Hub:\n",
    "- Hub_Customer\n",
    "- Hub_Order\n",
    "- Hub_Product\n",
    "- Hub_Seller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hub_Customer\n",
    "hub_customer = customers_df.select(\"customer_id\").distinct()\n",
    "hub_customer = generate_hash_key(hub_customer, [\"customer_id\"], \"hub_customer_key\")\n",
    "hub_customer = add_dv_metadata(hub_customer)\n",
    "\n",
    "# Salvar Hub_Customer como Delta\n",
    "hub_customer_path = f\"{delta_base_path}/hub_customer\"\n",
    "hub_customer.write.format(\"delta\").mode(\"overwrite\").save(hub_customer_path)\n",
    "\n",
    "# Hub_Order\n",
    "hub_order = orders_df.select(\"order_id\").distinct()\n",
    "hub_order = generate_hash_key(hub_order, [\"order_id\"], \"hub_order_key\")\n",
    "hub_order = add_dv_metadata(hub_order)\n",
    "\n",
    "# Salvar Hub_Order como Delta\n",
    "hub_order_path = f\"{delta_base_path}/hub_order\"\n",
    "hub_order.write.format(\"delta\").mode(\"overwrite\").save(hub_order_path)\n",
    "\n",
    "# Hub_Product\n",
    "hub_product = products_df.select(\"product_id\").distinct()\n",
    "hub_product = generate_hash_key(hub_product, [\"product_id\"], \"hub_product_key\")\n",
    "hub_product = add_dv_metadata(hub_product)\n",
    "\n",
    "# Salvar Hub_Product como Delta\n",
    "hub_product_path = f\"{delta_base_path}/hub_product\"\n",
    "hub_product.write.format(\"delta\").mode(\"overwrite\").save(hub_product_path)\n",
    "\n",
    "# Hub_Seller\n",
    "hub_seller = sellers_df.select(\"seller_id\").distinct()\n",
    "hub_seller = generate_hash_key(hub_seller, [\"seller_id\"], \"hub_seller_key\")\n",
    "hub_seller = add_dv_metadata(hub_seller)\n",
    "\n",
    "# Salvar Hub_Seller como Delta\n",
    "hub_seller_path = f\"{delta_base_path}/hub_seller\"\n",
    "hub_seller.write.format(\"delta\").mode(\"overwrite\").save(hub_seller_path)\n",
    "\n",
    "# Visualizar as tabelas Hub criadas\n",
    "print(\"Hub_Customer:\")\n",
    "spark.read.format(\"delta\").load(hub_customer_path).show(5)\n",
    "\n",
    "print(\"\\nHub_Order:\")\n",
    "spark.read.format(\"delta\").load(hub_order_path).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Criação das Tabelas Link\n",
    "\n",
    "Vamos criar as seguintes tabelas Link:\n",
    "- Link_Customer_Order\n",
    "- Link_Order_Product\n",
    "- Link_Product_Seller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link_Customer_Order\n",
    "# Juntar os dados necessários\n",
    "customer_order_df = orders_df.select(\"order_id\", \"customer_id\").distinct()\n",
    "\n",
    "# Juntar com os Hubs para obter as chaves\n",
    "customer_order_link = customer_order_df.join(\n",
    "    spark.read.format(\"delta\").load(hub_customer_path),\n",
    "    on=\"customer_id\"\n",
    ").join(\n",
    "    spark.read.format(\"delta\").load(hub_order_path),\n",
    "    on=\"order_id\"\n",
    ")\n",
    "\n",
    "# Gerar a chave composta do link\n",
    "customer_order_link = generate_hash_key(\n",
    "    customer_order_link,\n",
    "    [\"hub_customer_key\", \"hub_order_key\"],\n",
    "    \"link_customer_order_key\"\n",
    ")\n",
    "\n",
    "# Selecionar as colunas necessárias e adicionar metadados\n",
    "customer_order_link = customer_order_link.select(\n",
    "    \"link_customer_order_key\", \"hub_customer_key\", \"hub_order_key\", \"customer_id\", \"order_id\"\n",
    ")\n",
    "customer_order_link = add_dv_metadata(customer_order_link)\n",
    "\n",
    "# Salvar Link_Customer_Order como Delta\n",
    "link_customer_order_path = f\"{delta_base_path}/link_customer_order\"\n",
    "customer_order_link.write.format(\"delta\").mode(\"overwrite\").save(link_customer_order_path)\n",
    "\n",
    "# Link_Order_Product (via order_items)\n",
    "# Obter dados distintos\n",
    "order_product_df = order_items_df.select(\"order_id\", \"product_id\").distinct()\n",
    "\n",
    "# Juntar com os Hubs para obter as chaves\n",
    "order_product_link = order_product_df.join(\n",
    "    spark.read.format(\"delta\").load(hub_order_path),\n",
    "    on=\"order_id\"\n",
    ").join(\n",
    "    spark.read.format(\"delta\").load(hub_product_path),\n",
    "    on=\"product_id\"\n",
    ")\n",
    "\n",
    "# Gerar a chave composta do link\n",
    "order_product_link = generate_hash_key(\n",
    "    order_product_link,\n",
    "    [\"hub_order_key\", \"hub_product_key\"],\n",
    "    \"link_order_product_key\"\n",
    ")\n",
    "\n",
    "# Selecionar as colunas necessárias e adicionar metadados\n",
    "order_product_link = order_product_link.select(\n",
    "    \"link_order_product_key\", \"hub_order_key\", \"hub_product_key\", \"order_id\", \"product_id\"\n",
    ")\n",
    "order_product_link = add_dv_metadata(order_product_link)\n",
    "\n",
    "# Salvar Link_Order_Product como Delta\n",
    "link_order_product_path = f\"{delta_base_path}/link_order_product\"\n",
    "order_product_link.write.format(\"delta\").mode(\"overwrite\").save(link_order_product_path)\n",
    "\n",
    "# Link_Product_Seller (via order_items)\n",
    "# Obter dados distintos\n",
    "product_seller_df = order_items_df.select(\"product_id\", \"seller_id\").distinct()\n",
    "\n",
    "# Juntar com os Hubs para obter as chaves\n",
    "product_seller_link = product_seller_df.join(\n",
    "    spark.read.format(\"delta\").load(hub_product_path),\n",
    "    on=\"product_id\"\n",
    ").join(\n",
    "    spark.read.format(\"delta\").load(hub_seller_path),\n",
    "    on=\"seller_id\"\n",
    ")\n",
    "\n",
    "# Gerar a chave composta do link\n",
    "product_seller_link = generate_hash_key(\n",
    "    product_seller_link,\n",
    "    [\"hub_product_key\", \"hub_seller_key\"],\n",
    "    \"link_product_seller_key\"\n",
    ")\n",
    "\n",
    "# Selecionar as colunas necessárias e adicionar metadados\n",
    "product_seller_link = product_seller_link.select(\n",
    "    \"link_product_seller_key\", \"hub_product_key\", \"hub_seller_key\", \"product_id\", \"seller_id\"\n",
    ")\n",
    "product_seller_link = add_dv_metadata(product_seller_link)\n",
    "\n",
    "# Salvar Link_Product_Seller como Delta\n",
    "link_product_seller_path = f\"{delta_base_path}/link_product_seller\"\n",
    "product_seller_link.write.format(\"delta\").mode(\"overwrite\").save(link_product_seller_path)\n",
    "\n",
    "# Visualizar as tabelas Link criadas\n",
    "print(\"Link_Customer_Order:\")\n",
    "spark.read.format(\"delta\").load(link_customer_order_path).show(5)\n",
    "\n",
    "print(\"\\nLink_Order_Product:\")\n",
    "spark.read.format(\"delta\").load(link_order_product_path).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Criação das Tabelas Satellite\n",
    "\n",
    "Vamos criar as seguintes tabelas Satellite:\n",
    "- Sat_Customer_Details\n",
    "- Sat_Order_Details\n",
    "- Sat_Product_Details\n",
    "- Sat_Seller_Details\n",
    "- Sat_Order_Item_Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sat_Customer_Details\n",
    "# Obter os dados de clientes\n",
    "customer_details = customers_df\n",
    "customer_details = customer_details.join(\n",
    "    spark.read.format(\"delta\").load(hub_customer_path),\n",
    "    on=\"customer_id\"\n",
    ")\n",
    "\n",
    "# Gerar hashkey para os atributos descritivos (para detectar mudanças)\n",
    "attribute_columns = [\n",
    "    \"customer_unique_id\", \"customer_zip_code_prefix\", \n",
    "    \"customer_city\", \"customer_state\"\n",
    "]\n",
    "customer_details = generate_hash_key(\n",
    "    customer_details, \n",
    "    attribute_columns, \n",
    "    \"hashdiff\"\n",
    ")\n",
    "\n",
    "# Selecionar as colunas necessárias e adicionar metadados\n",
    "customer_details = customer_details.select(\n",
    "    \"hub_customer_key\", \"hashdiff\", \"customer_id\", *attribute_columns\n",
    ")\n",
    "customer_details = add_dv_metadata(customer_details)\n",
    "\n",
    "# Salvar Sat_Customer_Details como Delta\n",
    "sat_customer_details_path = f\"{delta_base_path}/sat_customer_details\"\n",
    "customer_details.write.format(\"delta\").mode(\"overwrite\").save(sat_customer_details_path)\n",
    "\n",
    "# Sat_Order_Details\n",
    "# Obter os dados de pedidos\n",
    "order_details = orders_df\n",
    "order_details = order_details.join(\n",
    "    spark.read.format(\"delta\").load(hub_order_path),\n",
    "    on=\"order_id\"\n",
    ")\n",
    "\n",
    "# Gerar hashkey para os atributos descritivos\n",
    "attribute_columns = [\n",
    "    \"order_status\", \"order_purchase_timestamp\", \"order_approved_at\",\n",
    "    \"order_delivered_carrier_date\", \"order_delivered_customer_date\", \n",
    "    \"order_estimated_delivery_date\"\n",
    "]\n",
    "order_details = generate_hash_key(\n",
    "    order_details, \n",
    "    attribute_columns, \n",
    "    \"hashdiff\"\n",
    ")\n",
    "\n",
    "# Selecionar as colunas necessárias e adicionar metadados\n",
    "order_details = order_details.select(\n",
    "    \"hub_order_key\", \"hashdiff\", \"order_id\", *attribute_columns\n",
    ")\n",
    "order_details = add_dv_metadata(order_details)\n",
    "\n",
    "# Salvar Sat_Order_Details como Delta\n",
    "sat_order_details_path = f\"{delta_base_path}/sat_order_details\"\n",
    "order_details.write.format(\"delta\").mode(\"overwrite\").save(sat_order_details_path)\n",
    "\n",
    "# Sat_Product_Details\n",
    "# Obter os dados de produtos\n",
    "product_details = products_df\n",
    "product_details = product_details.join(\n",
    "    spark.read.format(\"delta\").load(hub_product_path),\n",
    "    on=\"product_id\"\n",
    ")\n",
    "\n",
    "# Gerar hashkey para os atributos descritivos\n",
    "attribute_columns = [\n",
    "    \"product_category_name\", \"product_name_length\", \"product_description_length\",\n",
    "    \"product_photos_qty\", \"product_weight_g\", \"product_length_cm\", \n",
    "    \"product_height_cm\", \"product_width_cm\"\n",
    "]\n",
    "product_details = generate_hash_key(\n",
    "    product_details, \n",
    "    attribute_columns, \n",
    "    \"hashdiff\"\n",
    ")\n",
    "\n",
    "# Selecionar as colunas necessárias e adicionar metadados\n",
    "product_details = product_details.select(\n",
    "    \"hub_product_key\", \"hashdiff\", \"product_id\", *attribute_columns\n",
    ")\n",
    "product_details = add_dv_metadata(product_details)\n",
    "\n",
    "# Salvar Sat_Product_Details como Delta\n",
    "sat_product_details_path = f\"{delta_base_path}/sat_product_details\"\n",
    "product_details.write.format(\"delta\").mode(\"overwrite\").save(sat_product_details_path)\n",
    "\n",
    "# Sat_Seller_Details\n",
    "# Obter os dados de vendedores\n",
    "seller_details = sellers_df\n",
    "seller_details = seller_details.join(\n",
    "    spark.read.format(\"delta\").load(hub_seller_path),\n",
    "    on=\"seller_id\"\n",
    ")\n",
    "\n",
    "# Gerar hashkey para os atributos descritivos\n",
    "attribute_columns = [\n",
    "    \"seller_zip_code_prefix\", \"seller_city\", \"seller_state\"\n",
    "]\n",
    "seller_details = generate_hash_key(\n",
    "    seller_details, \n",
    "    attribute_columns, \n",
    "    \"hashdiff\"\n",
    ")\n",
    "\n",
    "# Selecionar as colunas necessárias e adicionar metadados\n",
    "seller_details = seller_details.select(\n",
    "    \"hub_seller_key\", \"hashdiff\", \"seller_id\", *attribute_columns\n",
    ")\n",
    "seller_details = add_dv_metadata(seller_details)\n",
    "\n",
    "# Salvar Sat_Seller_Details como Delta\n",
    "sat_seller_details_path = f\"{delta_base_path}/sat_seller_details\"\n",
    "seller_details.write.format(\"delta\").mode(\"overwrite\").save(sat_seller_details_path)\n",
    "\n",
    "# Sat_Order_Item_Details (para o Link_Order_Product)\n",
    "# Obter os dados de itens de pedido\n",
    "order_item_details = order_items_df\n",
    "order_item_details = order_item_details.join(\n",
    "    order_product_link,\n",
    "    on=[\"order_id\", \"product_id\"]\n",
    ")\n",
    "\n",
    "# Gerar hashkey para os atributos descritivos\n",
    "attribute_columns = [\n",
    "    \"order_item_id\", \"price\", \"freight_value\", \"shipping_limit_date\"\n",
    "]\n",
    "order_item_details = generate_hash_key(\n",
    "    order_item_details, \n",
    "    attribute_columns, \n",
    "    \"hashdiff\"\n",
    ")\n",
    "\n",
    "# Selecionar as colunas necessárias e adicionar metadados\n",
    "order_item_details = order_item_details.select(\n",
    "    \"link_order_product_key\", \"hashdiff\", \"order_id\", \"product_id\", *attribute_columns\n",
    ")\n",
    "order_item_details = add_dv_metadata(order_item_details)\n",
    "\n",
    "# Salvar Sat_Order_Item_Details como Delta\n",
    "sat_order_item_details_path = f\"{delta_base_path}/sat_order_item_details\"\n",
    "order_item_details.write.format(\"delta\").mode(\"overwrite\").save(sat_order_item_details_path)\n",
    "\n",
    "# Visualizar as tabelas Satellite criadas\n",
    "print(\"Sat_Customer_Details:\")\n",
    "spark.read.format(\"delta\").load(sat_customer_details_path).show(5)\n",
    "\n",
    "print(\"\\nSat_Order_Details:\")\n",
    "spark.read.format(\"delta\").load(sat_order_details_path).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Consultando o Modelo Data Vault\n",
    "\n",
    "Agora que temos nosso modelo Data Vault implementado, vamos realizar algumas consultas para demonstrar como ele pode ser usado para responder a perguntas de negócio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo 1: Contagem de pedidos por status\n",
    "print(\"Contagem de pedidos por status:\")\n",
    "spark.read.format(\"delta\").load(sat_order_details_path) \\\n",
    "    .groupBy(\"order_status\") \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .show()\n",
    "\n",
    "# Exemplo 2: Top 10 cidades com mais clientes\n",
    "print(\"Top 10 cidades com mais clientes:\")\n",
    "spark.read.format(\"delta\").load(sat_customer_details_path) \\\n",
    "    .groupBy(\"customer_state\", \"customer_city\") \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .show(10)\n",
    "\n",
    "# Exemplo 3: Consulta mais complexa para obter detalhes de pedidos com clientes e produtos\n",
    "print(\"Exemplo de Business Vault - Detalhes completos de pedidos:\")\n",
    "\n",
    "# Carregar os dados\n",
    "hub_order = spark.read.format(\"delta\").load(hub_order_path)\n",
    "sat_order = spark.read.format(\"delta\").load(sat_order_details_path)\n",
    "link_customer_order = spark.read.format(\"delta\").load(link_customer_order_path)\n",
    "hub_customer = spark.read.format(\"delta\").load(hub_customer_path)\n",
    "sat_customer = spark.read.format(\"delta\").load(sat_customer_details_path)\n",
    "\n",
    "# Construir a consulta Business Vault\n",
    "business_vault_query = hub_order.join(\n",
    "    sat_order,\n",
    "    on=\"hub_order_key\"\n",
    ").join(\n",
    "    link_customer_order,\n",
    "    on=\"hub_order_key\"\n",
    ").join(\n",
    "    hub_customer,\n",
    "    on=\"hub_customer_key\"\n",
    ").join(\n",
    "    sat_customer,\n",
    "    on=\"hub_customer_key\"\n",
    ")\n",
    "\n",
    "# Selecionar colunas interessantes\n",
    "result = business_vault_query.select(\n",
    "    \"order_id\", \n",
    "    \"order_status\", \n",
    "    \"order_purchase_timestamp\", \n",
    "    \"order_delivered_customer_date\",\n",
    "    \"customer_id\",\n",
    "    \"customer_city\",\n",
    "    \"customer_state\"\n",
    ")\n",
    "\n",
    "# Mostrar os resultados\n",
    "result.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Demonstração de Histórico (Historização)\n",
    "\n",
    "Uma das grandes vantagens do Data Vault é a capacidade de rastrear mudanças ao longo do tempo. Vamos simular uma atualização em alguns dados para demonstrar como isso funciona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular uma atualização no status de alguns pedidos\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "# Carregar pedidos atuais\n",
    "current_orders = spark.read.format(\"delta\").load(sat_order_details_path)\n",
    "\n",
    "# Selecionar alguns pedidos para atualizar (por exemplo, os 10 primeiros)\n",
    "orders_to_update = current_orders.limit(10)\n",
    "\n",
    "# Mudar o status para 'delivered'\n",
    "updated_orders = orders_to_update.withColumn(\"order_status\", F.lit(\"delivered\"))\n",
    "\n",
    "# Recalcular o hashdiff para detectar a mudança\n",
    "attribute_columns = [\n",
    "    \"order_status\", \"order_purchase_timestamp\", \"order_approved_at\",\n",
    "    \"order_delivered_carrier_date\", \"order_delivered_customer_date\", \n",
    "    \"order_estimated_delivery_date\"\n",
    "]\n",
    "updated_orders = generate_hash_key(\n",
    "    updated_orders, \n",
    "    attribute_columns, \n",
    "    \"hashdiff\"\n",
    ")\n",
    "\n",
    "# Adicionar nova data de carregamento para indicar que este é um novo registro\n",
    "updated_orders = updated_orders.withColumn(\"load_date\", F.current_timestamp())\n",
    "\n",
    "# Usar a operação MERGE do Delta Lake para adicionar os novos registros\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Carregar a tabela Delta existente\n",
    "deltaTable = DeltaTable.forPath(spark, sat_order_details_path)\n",
    "\n",
    "# Realizar a operação MERGE para adicionar os novos registros\n",
    "deltaTable.alias(\"target\").merge(\n",
    "    updated_orders.alias(\"updates\"),\n",
    "    \"target.hub_order_key = updates.hub_order_key AND target.hashdiff != updates.hashdiff\"\n",
    ").whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# Verificar o histórico agora (deve conter registros duplicados com datas diferentes)\n",
    "updated_orders_history = spark.read.format(\"delta\").load(sat_order_details_path)\n",
    "print(\"Total de registros após atualização:\", updated_orders_history.count())\n",
    "\n",
    "# Verificar os pedidos atualizados (ordenados por hub_order_key e load_date)\n",
    "updated_orders_history.filter(\n",
    "    F.col(\"hub_order_key\").isin(updated_orders.select(\"hub_order_key\").collect()[0][0])\n",
    ").orderBy(\"hub_order_key\", \"load_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Demonstração de Linhagem de Dados\n",
    "\n",
    "Outra vantagem importante do Data Vault é a linhagem de dados (rastreabilidade). Vamos demonstrar como podemos rastrear a origem dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de rastreabilidade - seguindo um pedido específico desde a fonte até seus relacionamentos\n",
    "# Escolher um order_id de exemplo\n",
    "sample_order_id = spark.read.format(\"delta\").load(hub_order_path).limit(1).select(\"order_id\").collect()[0][0]\n",
    "print(f\"Rastreando o pedido: {sample_order_id}\")\n",
    "\n",
    "# 1. Encontrar o Hub_Order\n",
    "order_hub = spark.read.format(\"delta\").load(hub_order_path).filter(F.col(\"order_id\") == sample_order_id)\n",
    "print(\"\\n1. Hub_Order:\")\n",
    "order_hub.show()\n",
    "\n",
    "hub_order_key = order_hub.select(\"hub_order_key\").collect()[0][0]\n",
    "\n",
    "# 2. Encontrar os detalhes no Satellite\n",
    "order_details = spark.read.format(\"delta\").load(sat_order_details_path).filter(F.col(\"hub_order_key\") == hub_order_key)\n",
    "print(\"\\n2. Sat_Order_Details:\")\n",
    "order_details.show()\n",
    "\n",
    "# 3. Encontrar o cliente relacionado via Link\n",
    "customer_link = spark.read.format(\"delta\").load(link_customer_order_path).filter(F.col(\"hub_order_key\") == hub_order_key)\n",
    "print(\"\\n3. Link_Customer_Order:\")\n",
    "customer_link.show()\n",
    "\n",
    "hub_customer_key = customer_link.select(\"hub_customer_key\").collect()[0][0]\n",
    "\n",
    "# 4. Encontrar os detalhes do cliente\n",
    "customer_details = spark.read.format(\"delta\").load(sat_customer_details_path).filter(F.col(\"hub_customer_key\") == hub_customer_key)\n",
    "print(\"\\n4. Sat_Customer_Details:\")\n",
    "customer_details.show()\n",
    "\n",
    "# 5. Encontrar os produtos relacionados via Link\n",
    "product_link = spark.read.format(\"delta\").load(link_order_product_path).filter(F.col(\"hub_order_key\") == hub_order_key)\n",
    "print(\"\\n5. Link_Order_Product:\")\n",
    "product_link.show()\n",
    "\n",
    "# Demonstrar que a linhagem completa do pedido está preservada e pode ser facilmente rastreada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusão\n",
    "\n",
    "Neste notebook, demonstramos:\n",
    "\n",
    "1. Como configurar um ambiente de modelagem Data Vault com PySpark e Delta Lake\n",
    "2. Como criar as diferentes tabelas do Data Vault (Hubs, Links e Satellites)\n",
    "3. Como consultar o modelo Data Vault para responder perguntas de negócio\n",
    "4. Como implementar historização de dados no Data Vault\n",
    "5. Como rastrear a linhagem dos dados através do modelo\n",
    "\n",
    "Isso fornece uma base sólida para implementar uma arquitetura Data Vault em seu próprio projeto, garantindo:\n",
    "- Flexibilidade para mudanças de requisitos\n",
    "- Escalabilidade para grandes volumes de dados\n",
    "- Rastreabilidade completa da linhagem de dados\n",
    "- Histórico auditável de todas as alterações\n",
    "\n",
    "O Data Vault é particularmente útil para Data Warehouses empresariais que precisam suportar requisitos em constante mudança e múltiplas fontes de dados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
