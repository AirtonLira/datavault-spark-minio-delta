# Projeto de Modelagem Data Vault com Apache Spark e Delta Lake

![Data Vault](https://img.shields.io/badge/Data%20Vault-2.0-blue)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.4.1-orange)
![Delta Lake](https://img.shields.io/badge/Delta%20Lake-2.4.0-green)
![Python](https://img.shields.io/badge/Python-3.10%2B-yellow)

Este projeto demonstra a implementaÃ§Ã£o prÃ¡tica de uma arquitetura Data Vault utilizando Apache Spark, Delta Lake e Minio como armazenamento de objetos. A modelagem Ã© aplicada ao conjunto de dados pÃºblico brasileiro de E-Commerce da Olist.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Conceitos do Data Vault](#conceitos-do-data-vault)
- [Tecnologias Utilizadas](#tecnologias-utilizadas)
- [Arquitetura do Projeto](#arquitetura-do-projeto)
- [PrÃ©-requisitos](#prÃ©-requisitos)
- [InstalaÃ§Ã£o e ExecuÃ§Ã£o](#instalaÃ§Ã£o-e-execuÃ§Ã£o)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Conjunto de Dados](#conjunto-de-dados)
- [SoluÃ§Ã£o de Problemas](#soluÃ§Ã£o-de-problemas)
- [ReferÃªncias](#referÃªncias)

## ğŸŒŸ VisÃ£o Geral

A arquitetura Data Vault Ã© uma metodologia de modelagem de dados projetada para:

- Fornecer um histÃ³rico auditÃ¡vel e completo de todos os dados
- Ser altamente adaptÃ¡vel a mudanÃ§as de requisitos de negÃ³cios
- Separar as regras de negÃ³cio dos dados brutos
- Escalar de forma eficiente para grandes volumes de dados

Este projeto implementa um ambiente completo para Data Vault usando:

- **Apache Spark**: Para processamento distribuÃ­do de dados
- **Delta Lake**: Para garantir transaÃ§Ãµes ACID e viagem no tempo
- **Minio**: Como armazenamento de objetos compatÃ­vel com S3
- **Jupyter Notebook**: Para desenvolvimento interativo e exploraÃ§Ã£o de dados

## ğŸ§© Conceitos do Data Vault

O Data Vault consiste em trÃªs tipos principais de componentes:

1. **Hubs**: Armazenam as chaves de negÃ³cio e identificadores Ãºnicos das entidades
   - Representam o "o quÃª" dos dados
   - ContÃªm um hash de chave de negÃ³cio (Hub Key) e a chave de negÃ³cio original

2. **Links**: Armazenam as relaÃ§Ãµes entre os hubs
   - Representam o "como" os dados estÃ£o conectados
   - ContÃªm referÃªncias (chaves estrangeiras) para os hubs que relacionam

3. **Satellites**: Armazenam os atributos descritivos dos hubs e links
   - Representam o "contexto" e os detalhes das entidades
   - SÃ£o datados e historicizados para rastrear mudanÃ§as ao longo do tempo

![Estrutura Data Vault](https://github.com/your-username/data-vault-spark-delta/raw/main/docs/data_vault_structure.png)

## ğŸ”§ Tecnologias Utilizadas

- **Python 3.10+**
- **Apache Spark 3.4.1**: Framework de processamento distribuÃ­do
- **Delta Lake 2.4.0**: Camada de armazenamento transacional para data lakes
- **Minio**: Armazenamento de objetos compatÃ­vel com S3
- **Docker & Docker Compose**: Para containerizaÃ§Ã£o e orquestraÃ§Ã£o
- **Jupyter Notebook**: Ambiente interativo para exploraÃ§Ã£o e modelagem
- **Poetry**: Gerenciamento de dependÃªncias Python

## ğŸ—ï¸ Arquitetura do Projeto

A arquitetura implementada neste projeto inclui:

- **Cluster Spark**: Um master e dois workers para processamento distribuÃ­do
- **Minio**: Para armazenar os dados brutos, simulando um data lake
- **Jupyter Notebook**: Para desenvolvimento interativo e execuÃ§Ã£o das transformaÃ§Ãµes
- **Volume de dados**: Mapeado entre o host e os containers para persistÃªncia

Os dados fluem atravÃ©s dos seguintes estÃ¡gios:

1. IngestÃ£o dos dados brutos no Minio
2. Leitura e processamento com Spark
3. Modelagem Data Vault (Hubs, Links, Satellites)
4. Armazenamento como tabelas Delta Lake
5. Consultas e anÃ¡lises sobre o modelo Data Vault

## ğŸ“‹ PrÃ©-requisitos

- [Docker](https://www.docker.com/get-started)
- [Docker Compose](https://docs.docker.com/compose/install/)
- Git

## ğŸš€ InstalaÃ§Ã£o e ExecuÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/data-vault-spark-delta.git
cd data-vault-spark-delta
```

### 2. Prepare os dados e configure o ambiente

Execute o script de preparaÃ§Ã£o para baixar o conjunto de dados e configurar o Minio:

```bash
chmod +x scripts/download_dataset.sh
./scripts/download_dataset.sh

# Configure o Minio (caso necessÃ¡rio)
chmod +x scripts/fix_minio_setup.sh
./scripts/fix_minio_setup.sh
```

### 3. Inicie os serviÃ§os com Docker Compose

```bash
docker-compose up -d
```

### 4. Acesse o Jupyter Notebook

Abra seu navegador e acesse:

```
http://localhost:8888
```

NÃ£o Ã© necessÃ¡rio senha ou token para acesso.

### 5. Acesse a interface do Minio (opcional)

Para explorar os dados armazenados no Minio:

```
http://localhost:9001
```

Credenciais padrÃ£o: 
- UsuÃ¡rio: `minioadmin`
- Senha: `minioadmin`

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ docker-compose.yml           # ConfiguraÃ§Ã£o dos serviÃ§os Docker
â”œâ”€â”€ jupyter/                     # ConfiguraÃ§Ã£o do Jupyter
â”‚   â””â”€â”€ Dockerfile               # Dockerfile personalizado para o Jupyter
â”œâ”€â”€ notebooks/                   # Notebooks Jupyter
â”‚   â””â”€â”€ data_vault_modeling.ipynb  # ImplementaÃ§Ã£o do Data Vault
â”œâ”€â”€ pyproject.toml               # ConfiguraÃ§Ã£o do Poetry e dependÃªncias
â”œâ”€â”€ README.md                    # Este arquivo
â”œâ”€â”€ scripts/                     # Scripts utilitÃ¡rios
â”‚   â”œâ”€â”€ download_dataset.sh      # Script para download dos dados
â”‚   â””â”€â”€ fix_minio_setup.sh       # Script para configuraÃ§Ã£o do Minio
â””â”€â”€ data/                        # DiretÃ³rio para armazenar dados (ignorado pelo Git)
    â”œâ”€â”€ raw/                     # Dados brutos em CSV
    â”œâ”€â”€ minio_data/              # Dados do Minio
    â””â”€â”€ delta/                   # Tabelas Delta Lake geradas
```

## ğŸ“Š Conjunto de Dados

Este projeto utiliza o conjunto de dados pÃºblico brasileiro de E-Commerce da Olist, que contÃ©m informaÃ§Ãµes sobre pedidos, produtos, clientes e vendedores. Este dataset inclui:

- **Pedidos**: Status, datas de compra, aprovaÃ§Ã£o e entrega
- **Clientes**: LocalizaÃ§Ã£o geogrÃ¡fica (cidade, estado)
- **Produtos**: Categorias, dimensÃµes, pesos
- **Vendedores**: LocalizaÃ§Ã£o geogrÃ¡fica
- **Itens de Pedido**: PreÃ§os, fretes, datas de envio

Este cenÃ¡rio Ã© ideal para demonstraÃ§Ã£o de modelagem Data Vault, pois contÃ©m mÃºltiplas entidades com relacionamentos diversos e atributos contextuais.

## ğŸ” SoluÃ§Ã£o de Problemas

### Problemas de conexÃ£o com o Minio

Se encontrar problemas ao tentar ler os dados do Minio, execute:

```bash
./scripts/fix_minio_setup.sh
```

Este script configura o bucket do Minio, carrega os arquivos e verifica se os JARs necessÃ¡rios para o S3A estÃ£o disponÃ­veis.

### Erros do S3A FileSystem no Spark

Se ocorrerem erros como `ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found`, adicione o seguinte cÃ³digo ao seu notebook:

```python
# Cole o conteÃºdo do arquivo minio_integration.py em uma cÃ©lula do notebook
```

### Alternativa: Leitura Local dos Dados

Se a integraÃ§Ã£o com o Minio continuar apresentando problemas, vocÃª pode optar por ler os dados diretamente do sistema de arquivos local. O cÃ³digo em `minio_integration.py` jÃ¡ inclui um fallback automÃ¡tico para isso.

## ğŸ“š ReferÃªncias

- [Data Vault Alliance](https://datavaultalliance.com/)
- [Delta Lake Documentation](https://docs.delta.io/)
- [Apache Spark Documentation](https://spark.apache.org/docs/)
- [Minio Documentation](https://docs.min.io/)
- [Dataset Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)
- [Data Vault 2.0 Methodology](https://www.data-vault.co.uk/data-vault-2-0-methodology/)

---


- Desenvolvido com â¤ï¸ de Airton Lira Junior para comunidade.
- MeuLinkedln: https://www.linkedin.com/in/airton-lira-junior-6b81a661/